{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\workspace\\practice\\env2\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: fugashi in c:\\workspace\\practice\\env2\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: ipadic in c:\\workspace\\practice\\env2\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\workspace\\practice\\env2\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\workspace\\practice\\env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: colorama in c:\\workspace\\practice\\env2\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\WorkSpace\\Practice\\env2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインストール\n",
    "%pip install transformers fugashi ipadic sentencepiece\n",
    "\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # モデル出力の最初の要素がトークン埋め込みを含む\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        for batch_idx in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"max_length\", max_length=512,\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WorkSpace\\Practice\\env2\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-v3 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-char-v3 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-large-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-large-japanese-char-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# テキストファイルを読み込み、各文をリストに格納\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentences = f.read().splitlines()\n",
    "    return sentences\n",
    "\n",
    "# モデルの初期化\n",
    "model1 = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\")\n",
    "model2 = SentenceBertJapanese(\"tohoku-nlp/bert-base-japanese\")\n",
    "model3 = SentenceBertJapanese(\"tohoku-nlp/bert-base-japanese-v3\")\n",
    "model4 = SentenceBertJapanese(\"tohoku-nlp/bert-base-japanese-char-v3\")\n",
    "model5 = SentenceBertJapanese(\"tohoku-nlp/bert-large-japanese-v2\")\n",
    "model6 = SentenceBertJapanese(\"tohoku-nlp/bert-large-japanese-char-v2\")\n",
    "\n",
    "\n",
    "\n",
    "# テキストファイルから文を読み込む\n",
    "file_path = \"E:\\実験\\実験結果/18.文章25\\文章25.txt\"  # 読み込むファイルのパス\n",
    "sentences = read_sentences_from_file(file_path)\n",
    "\n",
    "# 文をエンコード\n",
    "embeddings1 = model1.encode(sentences)\n",
    "embeddings2 = model2.encode(sentences)\n",
    "embeddings3 = model3.encode(sentences)\n",
    "embeddings4 = model4.encode(sentences)\n",
    "embeddings5 = model5.encode(sentences)\n",
    "embeddings6 = model6.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無論、今日のように学問が個々の諸学問に分かれる以前は、学問をすること自体が哲学をするのと同じ意味だった。\n",
      "このように、歴史的起源あるいは発生的観点から、哲学が全ての学問の根本であるのは事実である。\n",
      "しかし、哲学が学問の根本と呼べる本源的な理由は別にある。\n",
      "それは哲学が基本的に個々の学問の土台となっている根本的な概念や前提そのものに対して疑問を提起し、それらをテーマ的に扱うという事実に起因する。\n",
      "いくつかの例を挙げてみることにしよう。\n",
      "基本的に「科学」はこの世界で起こる、ある事象を“説明”することを目指す。\n",
      " しかし、もし誰かがこの世界で起きる、ある具体的な事象を科学的に説明しようとするのではなく、「科学的説明」自体が何かについて疑問を提起し、科学的理論がどのように「客観性」を確保できるかについて明らかにしようとするならば、それは科学ではなく、「科学哲学」である。\n",
      "自然科学に関する一例を挙げよう。\n",
      " 私たちは高校の化学の授業で、物質の根本単位は「原子」と学んできた。\n",
      " しかし、原子のような小さい物質は私たちの肉眼で“直接的に観察”することが不可能である。\n",
      "ただ、電子顕微鏡を通じてその存在をただ間接的に類推できるだけだ。\n",
      " それでは、「原子」というのは本当に存在するのだろうか。\n",
      "それとも、それはただ、ある事態を効果的に説明するために理論的に仮定されたフィクションに過ぎないのだろうか。\n",
      "もし、誰かがこのような質問に対して悩み始めたとしたら、その人は科学で前提にしている理論的対象に対する形而上学的、存在論的問題について悩んでいると見ることができる。\n",
      "それなら、原子というものが本当に存在すると仮定してみよう。\n",
      " ところが、先に述べたように、原子を直接的に肉眼で観察することが不可能ならば、原子というものが本当に存在するということを、私たちはどうやって知ることができるだろうか。\n",
      " 電子顕微鏡によって間接的に原子の存在を類推するだけで、私たちは原子が存在するということが本当に”分かる”と言えるだろうか。 \n",
      "このような疑問は、原子よりも小さい電子や微粒子の存在に対しても提起することができる。\n",
      " もし誰かがこのような問題について悩み始めたら、その人は科学で想定している理論的対象に対して認識論的に悩んでいると言える。\n",
      "これに似た疑問を社会科学分野にも適用させることができる。\n",
      " 一社会を構成する根本単位は個人だろうか、それとも社会構造だろうか。\n",
      " あるいは、ある社会的現象を説明するにあたって、社会と個人の中で「説明すること」とは何であり、「説明されること」とは何だろうか。 \n",
      "これは社会の基本構成単位、あるいは社会科学的説明の基本単位に対する形而上学的あるいは存在論的質問である。\n",
      "よく、この質問に対して、経済学者たちは「個人」と答え、社会学者たちは「社会構造」と答える。\n",
      " 社会の基本構成単位あるいは社会科学的説明の基本単位に対するこのような存在論的立場の相違は、つまり何が正しい社会科学的説明かという「方法論」の違いにつながる。 \n",
      "社会学では主に社会的構造がどのように個人の行動を決定するのかを明らかにしようとする「構造主義」的説明方式に従い、経済学では主にミクロ的な観点で各個人の個別的選好が集まってどのように集合的な社会的現象が発生するのかを明らかにしようとする「方法論的個人主義」的説明方式に従っている。 \n",
      "このような社会科学的方法論の持つ長所－短所は何で、どのような方法論が社会科学的な観点から最も妥当かを問うことは社会科学ではなく、まさに「社会科学の哲学」が行われているということである。\n",
      "このように、哲学はそれぞれの個別科学が基本的に仮定している根本的な概念、前提、学問的方法論そのものについて疑問を提起し、それらについてテーマ的に研究をするという点で全ての学問の根本と言える。 \n",
      "簡単に言うと、哲学は「学問に対する学問」だと言える。\n",
      "これを元に、私は個人的に哲学を次のように定義したいと思う。\n",
      "すなわち哲学とは、それぞれの個別学問が基本的に仮定している根本的な概念、定義、前提、方法論に対して認識論的、存在論的、価値論的、論理的問題を提起し、明確な言語分析と論理的分析を通じて、そのような問題を体系的に解決するために努める学問である、と。\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(sentences)):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717995822429657\n",
      "0.7272423505783081\n",
      "0.6650372743606567\n",
      "0.7151206731796265\n",
      "0.3032328188419342\n",
      "0.22000528872013092\n",
      "0.6526998281478882\n",
      "0.5056691765785217\n",
      "0.2958485186100006\n",
      "0.4588942527770996\n",
      "0.4985571801662445\n",
      "0.3285762071609497\n",
      "0.25539544224739075\n",
      "0.45571252703666687\n",
      "0.26467159390449524\n",
      "0.7296499013900757\n",
      "0.8287378549575806\n",
      "0.5560721755027771\n",
      "0.3734455704689026\n",
      "0.4850529134273529\n",
      "0.36868712306022644\n",
      "0.666556715965271\n",
      "0.6671707630157471\n",
      "0.6232078075408936\n",
      "0.5612947940826416\n",
      "0.6764847040176392\n",
      "0.5508853793144226\n",
      "0.5671570301055908\n",
      "0.7556020617485046\n",
      "0.637042760848999\n",
      "0.6241620182991028\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## コサイン類似度を計算して出力\n",
    "#for i in range(len(sentences) - 1):\n",
    "#    print(f'sentence {i+1} vs sentence {i+2}: {cos_sim(embeddings[i].numpy(), embeddings[i+1].numpy())}')\n",
    "\n",
    "# コサイン類似度を計算して出力\n",
    "for i in range(len(sentences) - 1):\n",
    "    print(f'{cos_sim(embeddings1[i].numpy(), embeddings1[i+1].numpy())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8971399664878845\n",
      "0.9105226993560791\n",
      "0.9269354939460754\n",
      "0.9155671000480652\n",
      "0.7874377965927124\n",
      "0.7896202802658081\n",
      "0.9304020404815674\n",
      "0.8186363577842712\n",
      "0.7984268665313721\n",
      "0.9097556471824646\n",
      "0.9113258123397827\n",
      "0.8532882332801819\n",
      "0.9050958752632141\n",
      "0.9219511151313782\n",
      "0.9018036127090454\n",
      "0.9199346303939819\n",
      "0.9541264772415161\n",
      "0.8849017024040222\n",
      "0.8844093680381775\n",
      "0.875779390335083\n",
      "0.801954448223114\n",
      "0.8979100584983826\n",
      "0.8867316246032715\n",
      "0.8845264911651611\n",
      "0.9015142321586609\n",
      "0.9426397681236267\n",
      "0.9354618191719055\n",
      "0.9455174803733826\n",
      "0.8760387301445007\n",
      "0.8752005100250244\n",
      "0.8787786960601807\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences) - 1):\n",
    "    print(f'{cos_sim(embeddings2[i].numpy(), embeddings2[i+1].numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9398021697998047\n",
      "0.9503487944602966\n",
      "0.9559738636016846\n",
      "0.9332430958747864\n",
      "0.7878365516662598\n",
      "0.801395833492279\n",
      "0.9187604188919067\n",
      "0.8410629630088806\n",
      "0.8662056922912598\n",
      "0.9153521656990051\n",
      "0.9322462677955627\n",
      "0.8648903965950012\n",
      "0.9116213917732239\n",
      "0.9113568067550659\n",
      "0.8991959691047668\n",
      "0.9246852993965149\n",
      "0.9575814008712769\n",
      "0.9194684624671936\n",
      "0.9082335233688354\n",
      "0.9032925963401794\n",
      "0.8349844813346863\n",
      "0.9064933061599731\n",
      "0.9156196713447571\n",
      "0.9196091890335083\n",
      "0.9315834045410156\n",
      "0.9516934752464294\n",
      "0.9445369839668274\n",
      "0.957012414932251\n",
      "0.9247029423713684\n",
      "0.9280030131340027\n",
      "0.9122307896614075\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences) - 1):\n",
    "    print(f'{cos_sim(embeddings3[i].numpy(), embeddings3[i+1].numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9257162809371948\n",
      "0.9472770690917969\n",
      "0.9692095518112183\n",
      "0.947005569934845\n",
      "0.839110791683197\n",
      "0.8563038110733032\n",
      "0.9258238673210144\n",
      "0.8276437520980835\n",
      "0.886405348777771\n",
      "0.9493907690048218\n",
      "0.9553306698799133\n",
      "0.889599084854126\n",
      "0.9253969192504883\n",
      "0.9459165930747986\n",
      "0.9343342185020447\n",
      "0.9371994137763977\n",
      "0.9754616022109985\n",
      "0.9450754523277283\n",
      "0.946420431137085\n",
      "0.9450122714042664\n",
      "0.8869732618331909\n",
      "0.9455122947692871\n",
      "0.9394050240516663\n",
      "0.9390310645103455\n",
      "0.9390324950218201\n",
      "0.9668002128601074\n",
      "0.9594862461090088\n",
      "0.964759111404419\n",
      "0.9241932034492493\n",
      "0.9367245435714722\n",
      "0.9049847722053528\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences) - 1):\n",
    "    print(f'{cos_sim(embeddings4[i].numpy(), embeddings4[i+1].numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9155191779136658\n",
      "0.9219727516174316\n",
      "0.9439801573753357\n",
      "0.9203893542289734\n",
      "0.7713755369186401\n",
      "0.7771182060241699\n",
      "0.8940684199333191\n",
      "0.8465381860733032\n",
      "0.8006554245948792\n",
      "0.8911910653114319\n",
      "0.9189766049385071\n",
      "0.8420080542564392\n",
      "0.8895239233970642\n",
      "0.9142545461654663\n",
      "0.8782067894935608\n",
      "0.9200230240821838\n",
      "0.9482157826423645\n",
      "0.9023969769477844\n",
      "0.8768181204795837\n",
      "0.9076787233352661\n",
      "0.850246250629425\n",
      "0.8428640961647034\n",
      "0.8673362135887146\n",
      "0.9164651036262512\n",
      "0.9266574382781982\n",
      "0.9432199001312256\n",
      "0.9237576127052307\n",
      "0.9431870579719543\n",
      "0.9037277698516846\n",
      "0.9041284918785095\n",
      "0.8592539429664612\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences) - 1):\n",
    "    print(f'{cos_sim(embeddings5[i].numpy(), embeddings5[i+1].numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9150168895721436\n",
      "0.9271034598350525\n",
      "0.9600526094436646\n",
      "0.9244741797447205\n",
      "0.7977563142776489\n",
      "0.8157901167869568\n",
      "0.9142224788665771\n",
      "0.8416613936424255\n",
      "0.8589282631874084\n",
      "0.9125257730484009\n",
      "0.9247131943702698\n",
      "0.8445374369621277\n",
      "0.8923085331916809\n",
      "0.9235914349555969\n",
      "0.8955807685852051\n",
      "0.9149519801139832\n",
      "0.9587867856025696\n",
      "0.9158143997192383\n",
      "0.8982867002487183\n",
      "0.916883111000061\n",
      "0.8736933469772339\n",
      "0.928999662399292\n",
      "0.921089231967926\n",
      "0.9220705628395081\n",
      "0.9244064092636108\n",
      "0.9546757340431213\n",
      "0.9443618059158325\n",
      "0.9463220834732056\n",
      "0.922421395778656\n",
      "0.911771297454834\n",
      "0.8723496794700623\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences) - 1):\n",
    "    print(f'{cos_sim(embeddings6[i].numpy(), embeddings6[i+1].numpy())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
