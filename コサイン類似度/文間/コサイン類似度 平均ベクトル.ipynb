{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\workspace\\practice\\env2\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: fugashi in c:\\workspace\\practice\\env2\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: ipadic in c:\\workspace\\practice\\env2\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\workspace\\practice\\env2\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\workspace\\practice\\env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: colorama in c:\\workspace\\practice\\env2\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインストール\n",
    "%pip install transformers fugashi ipadic sentencepiece\n",
    "\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # モデル出力の最初の要素がトークン埋め込みを含む\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        for batch_idx in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"max_length\", max_length=512,\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "# テキストファイルを読み込み、各文をリストに格納\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentences = f.read().splitlines()\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WorkSpace\\Practice\\env2\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-v3 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-char-v3 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-large-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-large-japanese-char-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# モデルの初期化\n",
    "model1 = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\")\n",
    "model2 = SentenceBertJapanese(\"tohoku-nlp/bert-base-japanese\")\n",
    "model3 = SentenceBertJapanese(\"tohoku-nlp/bert-base-japanese-v3\")\n",
    "model4 = SentenceBertJapanese(\"tohoku-nlp/bert-base-japanese-char-v3\")\n",
    "model5 = SentenceBertJapanese(\"tohoku-nlp/bert-large-japanese-v2\")\n",
    "model6 = SentenceBertJapanese(\"tohoku-nlp/bert-large-japanese-char-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文章22.txt', '文章23.txt', '文章24.txt', '文章25.txt', '文章01.txt', '文章02.txt', '文章04.txt', '文章06.txt', '文章07.txt', '文章09.txt', '文章10.txt', '文章11.txt', '文章14.txt', '文章15.txt', '文章17.txt', '文章18.txt', '文章19.txt', '文章21.txt', '文章27.txt', '文章29.txt', '文章33.txt', '文章36.txt', '文章42.txt', '文章45.txt', '文章46.txt', '文章26.txt']\n",
      "22を記録しました。\n",
      "23を記録しました。\n",
      "24を記録しました。\n",
      "25を記録しました。\n",
      "01を記録しました。\n",
      "02を記録しました。\n",
      "04を記録しました。\n",
      "06を記録しました。\n",
      "07を記録しました。\n",
      "09を記録しました。\n",
      "10を記録しました。\n",
      "11を記録しました。\n",
      "14を記録しました。\n",
      "15を記録しました。\n",
      "17を記録しました。\n",
      "18を記録しました。\n",
      "19を記録しました。\n",
      "21を記録しました。\n",
      "27を記録しました。\n",
      "29を記録しました。\n",
      "33を記録しました。\n",
      "36を記録しました。\n",
      "42を記録しました。\n",
      "45を記録しました。\n",
      "46を記録しました。\n",
      "26を記録しました。\n"
     ]
    }
   ],
   "source": [
    "for _,_, files in os.walk(\"E:\\実験\\テキスト/test\"):\n",
    "    print(files)\n",
    "    for i in range(0, len(files)):\n",
    "        file = str(files[i])\n",
    "        num = file[-6:-4]\n",
    "        # print(num)\n",
    "\n",
    "        file_path = \"E:\\実験\\テキスト/test/文章\" + str(num) + \".txt\"\n",
    "        save_path = \"C:\\WorkSpace\\Practice\\コサイン類似度/avecossim_result\" + str(num) + \".csv\"\n",
    "        sentences = read_sentences_from_file(file_path)\n",
    "\n",
    "        # 文をエンコード\n",
    "        embeddings1 = model1.encode(sentences)\n",
    "        embeddings2 = model2.encode(sentences)\n",
    "        embeddings3 = model3.encode(sentences)\n",
    "        embeddings4 = model4.encode(sentences)\n",
    "        embeddings5 = model5.encode(sentences)\n",
    "        embeddings6 = model6.encode(sentences)\n",
    "\n",
    "        total_vec1=0\n",
    "        ave_vec1 =0\n",
    "        for j in range(0, len(embeddings1)):\n",
    "            total_vec1 += embeddings1[j] \n",
    "            ave_vec1 = total_vec1 / len(embeddings1)\n",
    "\n",
    "        total_vec2=0\n",
    "        ave_vec2 =0\n",
    "        for j in range(0, len(embeddings2)):\n",
    "            total_vec2 += embeddings2[j] \n",
    "            ave_vec2 = total_vec2 / len(embeddings2)\n",
    "\n",
    "        total_vec3=0\n",
    "        ave_vec3 =0\n",
    "        for j in range(0, len(embeddings3)):\n",
    "            total_vec3 += embeddings3[j] \n",
    "            ave_vec3 = total_vec3 / len(embeddings3)\n",
    "\n",
    "        total_vec4=0\n",
    "        ave_vec4 =0\n",
    "        for j in range(0, len(embeddings4)):\n",
    "            total_vec4 += embeddings4[j] \n",
    "            ave_vec4 = total_vec4 / len(embeddings4)\n",
    "\n",
    "        total_vec5=0\n",
    "        ave_vec5 =0\n",
    "        for j in range(0, len(embeddings5)):\n",
    "            total_vec5 += embeddings5[j] \n",
    "            ave_vec5 = total_vec5 / len(embeddings5)\n",
    "\n",
    "        total_vec6=0\n",
    "        ave_vec6 =0\n",
    "        for j in range(0, len(embeddings6)):\n",
    "            total_vec6 += embeddings6[j] \n",
    "            ave_vec6 = total_vec6 / len(embeddings6)\n",
    "\n",
    "\n",
    "        avecossim_result1 = []\n",
    "        avecossim_result2 = []\n",
    "        avecossim_result3 = []\n",
    "        avecossim_result4 = []\n",
    "        avecossim_result5 = []\n",
    "        avecossim_result6 = []\n",
    "\n",
    "        for k in range(0, len(sentences)):\n",
    "            avecossim_result1.append(cos_sim(ave_vec1.numpy(), embeddings1[k].numpy()))\n",
    "            avecossim_result2.append(cos_sim(ave_vec2.numpy(), embeddings2[k].numpy()))\n",
    "            avecossim_result3.append(cos_sim(ave_vec3.numpy(), embeddings3[k].numpy()))\n",
    "            avecossim_result4.append(cos_sim(ave_vec4.numpy(), embeddings4[k].numpy()))\n",
    "            avecossim_result5.append(cos_sim(ave_vec5.numpy(), embeddings5[k].numpy()))\n",
    "            avecossim_result6.append(cos_sim(ave_vec6.numpy(), embeddings6[k].numpy()))\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\" : avecossim_result1,\n",
    "            \"tohoku-nlp/bert-base-japanese\" : avecossim_result2,\n",
    "            \"tohoku-nlp/bert-base-japanese-v3\" : avecossim_result3,\n",
    "            \"tohoku-nlp/bert-base-japanese-char-v3\" : avecossim_result4,\n",
    "            \"tohoku-nlp/bert-large-japanese-v2\" : avecossim_result5,\n",
    "            \"tohoku-nlp/bert-large-japanese-char-v2\" : avecossim_result6\n",
    "        })\n",
    "\n",
    "        df.to_csv(save_path, mode=\"a\", header=True, index=False)\n",
    "\n",
    "        print(f\"{num}を記録しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彼らが文明の証とみなしていたものは、農業による食料生産の開始、製鉄をはじめとする治金技術の発達、さまざまな技術的発明、集権的な集団組織の確立、そして文字の発明などである。\n",
      "これらの要素のうち、文字の発明は特定の地域で起こったものと昔から考えられてきた。\n",
      "たとえばイスラム圏の拡大や、ヨーロッパ人の植民地支配がはじまるまで、オーストラリア、太平洋諸島、赤道アフリカの人びとは、文字というものを持っていなかった。\n",
      "南北アメリカ大陸においても、中米の一部のごく狭い地域を除いて、文字を持つようになったのはヨーロッパ人の植民地支配が確立されてからのことである。\n",
      "文字の使用地域が世界的に極端に限定されていたことから、自分たちを文明人と思い込んでいた人びとは、文字を使える能力こそ、「野蛮人」や「未開人」に対する「文明人」の優越性を示すもっとも顕著なちがいだと考えていた。\n",
      "文字は、はるか遠く離れた世界についての知識をわれわれに教えてくれる。\n",
      "昔の出来事をわれわれに伝えてくれる。\n",
      "また、大量の知識を正確に伝達することができる。\n",
      "もちろん、インカ人たちのように、文字を持たずして大帝国を治めることができた人びともいた。\n",
      "フン族相手に大敗したローマ軍のように、「文明人」が「野蛮人」に常に勝利できたわけではない。\n",
      "しかし、文字は知識をもたらす。\n",
      "ヨーロッパ人が南北アメリカ大陸、シベリア、そしてオーストラリアを征服したことは、文字が知識をもたらし、知識が力をもたらすことを如実に示している。\n",
      "文字は、武器、病原菌、そして集権化された政治機構などとともに、近代ヨーロッパ人の海外遠征に付随して、世界のさまざまな場所に伝達していった。\n",
      "船隊を派遣した君主や大商人は、文書をしたため、命令や指示を伝えた。\n",
      "そうした船隊は、先遣隊の残した海図や航海誌を手がかりに進路をとった。\n",
      "遠征隊の記録は、莫大な富や肥沃な土地について語り、さらなる遠征の旅へと人びとをかりたてた。\n",
      "これらの記録は、あとにつづく者に、どのような環境が待ち受けているかを伝えた。\n",
      "征服者が現地に誕生させた帝国の統治は、文字を使っておこなわれた。\n",
      "もちろん、文字を持たなかった社会でも、さまざまな手段で情報は伝達されていた。\n",
      "しかし、文字は、情報をはるかに容易に、はるかに正確に、はるかに説得力のあるかたちで伝達できるようにした。\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(sentences)):\n",
    "    print(sentences[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
