{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\WorkSpace\\Practice\\env2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\workspace\\practice\\env2\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: fugashi in c:\\workspace\\practice\\env2\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: ipadic in c:\\workspace\\practice\\env2\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\workspace\\practice\\env2\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\workspace\\practice\\env2\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\workspace\\practice\\env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\workspace\\practice\\env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: colorama in c:\\workspace\\practice\\env2\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\workspace\\practice\\env2\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers fugashi ipadic sentencepiece\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "            #                                truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"max_length\", max_length=512,\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WorkSpace\\Practice\\env2\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens\")\n",
    "#model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\")\n",
    "#model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-en-mean-tokens\")\n",
    "#model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-en-mean-tokens-v2\")\n",
    "model = SentenceBertJapanese(\"tohoku-nlp/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"日本の風土の特質が日本の歴史と文化に大きな影響を与える。\"\n",
    "s2 = \"島国の日本は、四周を海にかこまれ世界から隔てられている。\"\n",
    "s3 = \"この地理的条件によって、日本は外敵の侵略と異民族の支配がなく、周囲から文化や技術などを吸収し、「島国」の中でそれを融和させて、独自の文化を磨きあげ築きあげてきたのである。\"\n",
    "s4 = \"外国との交通が海に隔てられ、発信的な文化型は形成しにくいので、自ら外国の文化を積極的に受けいれる受信的な文化型を形成してきた。\"\n",
    "s5 = \"日本文化の本質は受信文化である。\"\n",
    "s6 = \"外来文化の受信能力が世界一である日本から発信することはほとんどない。\"\n",
    "s7 = \"日本は外来文化を選択して受信するだけにとどまらず、さらに受信した外来文化を巧みに融合して日本化する。\"\n",
    "s8 = \"古代朝鮮や中国から漢字や儒教、仏教、道教など宗教思想を導入し、近代欧米の新文明を吸収し、更にそれを自国に適応しようと、工夫に努めた。\"\n",
    "#s8 = \"豊臣秀吉は偉大な武家だった。\"\n",
    "s9 = \"受信文化の特徴として、選択の可能性があること。\"\n",
    "s10 = \"日本は海に守られて外来侵略と異民族の支配がないため、受け入れる側の意識や都合を無視されるような押し付け、強制的な外来文化の受信は全くない。\"\n",
    "s = model.encode([s1, s2, s3, s4, s5, s6, s7, s8, s9, s10])\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 vs s2:0.8407385349273682\n",
      "s2 vs s3:0.8795011639595032\n",
      "s3 vs s4:0.9389750361442566\n",
      "s4 vs s5:0.8583738207817078\n",
      "s5 vs s6:0.8412156105041504\n",
      "s6 vs s7:0.886039674282074\n",
      "s7 vs s8:0.9106302261352539\n",
      "s8 vs s9:0.8608620762825012\n",
      "s9 vs s10:0.8704507946968079\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f's1 vs s2:{cos_sim(s[0].numpy(), s[1].numpy())}')\n",
    "print(f's2 vs s3:{cos_sim(s[1].numpy(), s[2].numpy())}')\n",
    "print(f's3 vs s4:{cos_sim(s[2].numpy(), s[3].numpy())}')\n",
    "print(f's4 vs s5:{cos_sim(s[3].numpy(), s[4].numpy())}')\n",
    "print(f's5 vs s6:{cos_sim(s[4].numpy(), s[5].numpy())}')\n",
    "print(f's6 vs s7:{cos_sim(s[5].numpy(), s[6].numpy())}')\n",
    "print(f's7 vs s8:{cos_sim(s[6].numpy(), s[7].numpy())}')\n",
    "print(f's8 vs s9:{cos_sim(s[7].numpy(), s[8].numpy())}')\n",
    "print(f's9 vs s10:{cos_sim(s[8].numpy(), s[9].numpy())}')\n",
    "#print(f's9 vs s10:{cos_sim(s[10].numpy(), s[11].numpy())}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
