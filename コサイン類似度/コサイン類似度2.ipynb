{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [51 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "c:\\WorkSpace\\Practice\\env2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.7.0 fugashi ipadic\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "            #                                truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"max_length\", max_length=512,\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'BertJapaneseTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceBertJapanese(\"sonoisa/sentence-bert-base-ja-mean-tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_a = \"ゴッドファーザー\"\n",
    "movie_a_comment_1 = \"在米イタリア人による，..........皮肉な話だ。\"\n",
    "\n",
    "movie_a_comment_2 = \"華やかな音楽も届かない書斎では、.....感じるからだろう。\"\n",
    "movie_a_comment_3 = \"まず、こういう映画を毛嫌いする方もいると思いますし、.....と個人的に思っています。\"\n",
    "movie_a_vectors = model.encode([movie_a_comment_1, movie_a_comment_2, movie_a_comment_3])\n",
    "movie_a_avg_vector = (movie_a_vectors[0].numpy() + movie_a_vectors[1].numpy() + movie_a_vectors[2].numpy()) / 3\n",
    "                                                    \n",
    "movie_b = \"スパイダーマン\"\n",
    "movie_b_comment_1 = \"映画「スパイダーマン」は、他のアメコミ映画とは明らか....のは俺だけではあるまい。\"\n",
    "movie_b_comment_2 = \"この映画の魅力…それは.....過ぎないのだから。\"\n",
    "movie_b_comment_3 = \"※注意。....、いやなんとなく。\"\n",
    "movie_b_vectors = model.encode([movie_b_comment_1, movie_b_comment_2, movie_b_comment_3])\n",
    "movie_b_avg_vector = (movie_b_vectors[0].numpy() + movie_b_vectors[1].numpy() + movie_b_vectors[2].numpy()) / 3\n",
    "                                                    \n",
    "movie_c = \"映画クレヨンしんちゃん　嵐を呼ぶモーレツ！オトナ帝国の逆襲\"\n",
    "movie_c_comment_1 = \"子供向けじゃあない....育った大人なんて、ゾっとする。\"\n",
    "movie_c_comment_2 = \"今まで観なくて....大事なのかもしれませんね\"\n",
    "movie_c_comment_3 = \"ノスタルジーって、....笑えて泣けた。心から！\"\n",
    "movie_c_vectors = model.encode([movie_c_comment_1, movie_c_comment_2, movie_c_comment_3])\n",
    "movie_c_avg_vector = (movie_c_vectors[0].numpy() + movie_c_vectors[1].numpy() + movie_c_vectors[2].numpy()) / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vsゴッドファーザー_平均:0.4020501673221588\n",
      "vsゴッドファーザー_コメント1:0.22076258063316345\n",
      "vsゴッドファーザー_コメント2:0.2961881458759308\n",
      "vsゴッドファーザー_コメント3:0.39889833331108093\n",
      "vsスパイダーマン_平均:0.4823598265647888\n",
      "vsスパイダーマン_コメント1:0.21828965842723846\n",
      "vsスパイダーマン_コメント2:0.47695136070251465\n",
      "vsスパイダーマン_コメント3:0.37502574920654297\n",
      "vs映画クレヨンしんちゃん　嵐を呼ぶモーレツ！オトナ帝国の逆襲_平均:0.6023193001747131\n",
      "vs映画クレヨンしんちゃん　嵐を呼ぶモーレツ！オトナ帝国の逆襲_コメント1:0.29586368799209595\n",
      "vs映画クレヨンしんちゃん　嵐を呼ぶモーレツ！オトナ帝国の逆襲_コメント2:0.28229811787605286\n",
      "vs映画クレヨンしんちゃん　嵐を呼ぶモーレツ！オトナ帝国の逆襲_コメント3:0.7783889770507812\n"
     ]
    }
   ],
   "source": [
    "comment = model.encode([\"笑って泣ける映画です\"])\n",
    "\n",
    "print(f'vs{movie_a}_平均:{cos_sim(movie_a_avg_vector, comment[0])}')\n",
    "print(f'vs{movie_a}_コメント1:{cos_sim(movie_a_vectors[0].numpy(), comment[0])}')\n",
    "print(f'vs{movie_a}_コメント2:{cos_sim(movie_a_vectors[1].numpy(), comment[0])}')\n",
    "print(f'vs{movie_a}_コメント3:{cos_sim(movie_a_vectors[2].numpy(), comment[0])}')\n",
    "\n",
    "print(f'vs{movie_b}_平均:{cos_sim(movie_b_avg_vector, comment[0])}')\n",
    "print(f'vs{movie_b}_コメント1:{cos_sim(movie_b_vectors[0].numpy(), comment[0])}')\n",
    "print(f'vs{movie_b}_コメント2:{cos_sim(movie_b_vectors[1].numpy(), comment[0])}')\n",
    "print(f'vs{movie_b}_コメント3:{cos_sim(movie_b_vectors[2].numpy(), comment[0])}')\n",
    "\n",
    "print(f'vs{movie_c}_平均:{cos_sim(movie_c_avg_vector, comment[0])}')\n",
    "print(f'vs{movie_c}_コメント1:{cos_sim(movie_c_vectors[0].numpy(), comment[0])}')\n",
    "print(f'vs{movie_c}_コメント2:{cos_sim(movie_c_vectors[1].numpy(), comment[0])}')\n",
    "print(f'vs{movie_c}_コメント3:{cos_sim(movie_c_vectors[2].numpy(), comment[0])}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
